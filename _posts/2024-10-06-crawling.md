---
layout: single
title: "BeautifulSoup: 웹 스크래핑 라이브러리"
categories: python
tag: [python, blog, BeautifulSoup, 프로그래머스4기]
search: true
typora-root-url: ../

---



**[BeautifulSoup]**[개념 및 실습](https://park-chanyeong.github.io)
{: .notice--primary}



#  1. BeautifulSoup란? 

---

![blog2](/images/2024-10-06-crawling/blog2.png)

파이썬과 웹 크롤링에 관심이 있다면, **BeautifulSoup**이라는 이름을 반드시 들어봤을 겁니다. 

이름은 아름다운 수프지만, 사실 웹페이지의 HTML을 뜨끈하게 '해체'하는 도구죠. 

이 도구는 웹페이지에서 필요한 정보를 스크랩하고자 할 때 요리처럼 사용됩니다. 

이름하여 '국밥' 라이브러리 🍜 

쉽게 말해, BeautifulSoup은 **HTML이나 XML 파일을 파싱**해서 데이터를 추출하는 웹 스크래핑 라이브러리입니다. 이를 통해 웹에서 데이터를 가져오고 분석할 수 있습니다.

# 2. 준비물 🛠

---



## 2-1. BeautifulSoup 설치 

아무리 맛있는 요리를 하려 해도 재료가 없으면 안 되겠죠? 먼저 BeautifulSoup을 설치해야 합니다. 간단하게 `pip` 명령어를 사용해 설치해 봅시다.

```bash
pip install beautifulsoup4
```



## 2-2. 필수 라이브러리

웹 크롤링에는 BeautifulSoup 외에도 도와줄 친구들이 필요합니다. **requests** 라이브러리를 사용하여 웹 페이지의 HTML을 가져오고, BeautifulSoup을 사용해 이를 파싱합니다.

```bash
pip install requests
```





# 3. 실습: Python으로 웹 크롤링하기

---



## 3-1. 간단한 예제

이제 본격적으로 크롤링을 시작해보죠. 오늘의 국밥은 'BeautifulSoup을 이용해 웹페이지에서 제목(title)을 가져오기'입니다.

```python
# 필수 라이브러리 임포트
import requests  # 웹 페이지 요청을 보내기 위한 라이브러리
from bs4 import BeautifulSoup  # HTML 파싱을 위한 라이브러리

# 크롤링할 웹 페이지 주소
url = "https://example.com"

# 웹 페이지의 HTML 소스를 가져옵니다.
response = requests.get(url)

# BeautifulSoup 객체 생성
soup = BeautifulSoup(response.text, 'html.parser')

# 웹 페이지의 <title> 태그에 있는 내용을 가져옵니다.
title = soup.title.string

# 제목 출력
print(f"이 웹 페이지의 제목은 '{title}'입니다.")

```

### 주요 설명:

- **requests.get(url)**: 해당 URL의 웹 페이지 내용을 가져옵니다. 그 결과를 `response` 객체에 저장하죠.
- **BeautifulSoup(response.text, 'html.parser')**: BeautifulSoup 객체를 생성하고, 가져온 HTML을 파싱합니다.
- **soup.title.string**: 파싱된 HTML에서 `<title>` 태그 안의 텍스트를 추출합니다.

이 코드를 실행하면, "이 웹 페이지의 제목은 'Example Domain'입니다." 같은 출력이 나오게 됩니다. 이걸로 기본 크롤링은 성공! 🎉



# 4. 웹 페이지 정보 더 많이 가져오기

---



이제 다양한 HTML 요소도 함께 가져와 봅시다.

## 4-1. 특정 태그의 모든 요소 가져오기

이번엔 `<a>` 태그, 즉 링크를 모두 가져와 봅시다. BeautifulSoup은 이렇게 다양한 태그를 쉽게 찾아내는 기능도 제공합니다.

```python
# <a> 태그를 모두 찾아서 리스트로 반환
links = soup.find_all('a')

# 링크 출력
for link in links:
    print(link.get('href'))  # 각 <a> 태그의 href 속성을 출력
```

이 코드를 실행하면 해당 웹 페이지의 모든 링크가 출력됩니다. 쉽게 말해, 링크 사냥꾼이 된 거죠! 🎯



# 5. 기타 활용 예시

---



BeautifulSoup은 이외에도 다양한 방법으로 HTML을 파싱할 수 있습니다.

## 5-1. 클래스명으로 요소 찾기

특정 클래스명을 가진 요소를 찾고 싶다면 이렇게 할 수 있습니다.

```python
# 클래스명이 'example'인 div 태그 찾기
divs = soup.find_all('div', class_='example')

for div in divs:
    print(div.text)  # 각 div 태그의 텍스트 출력
```

클래스명을 기준으로 찾을 수 있어서 매우 유용하죠. 이제는 '데이터 추출 마스터'라고 불러도 될 정도입니다! 🏆

---



# 6. 마무리: 스크래핑할 때 주의할 점

---



BeautifulSoup은 정말 강력한 도구지만, **웹 크롤링 시에는 주의할 점**도 많습니다. 특히 **웹사이트의 로봇 배제 표준(robots.txt)**을 꼭 확인하세요. 이 파일은 웹사이트가 허용하는 크롤러 범위를 지정합니다. 허용되지 않는 곳을 무단으로 크롤링하다가 문제가 생기면 인생 힘들어질 수도..🙅‍♂️